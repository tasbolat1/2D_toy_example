{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7760594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPyTorch Imports\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP, IndependentModelList\n",
    "from gpytorch.means import ConstantMean, MultitaskMean\n",
    "from gpytorch.kernels import ScaleKernel, MultitaskKernel\n",
    "from gpytorch.kernels import RBFKernel, RBFKernel, ProductKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood, LikelihoodList, MultitaskGaussianLikelihood\n",
    "from gpytorch.mlls import SumMarginalLogLikelihood, ExactMarginalLogLikelihood\n",
    "from gpytorch.distributions import MultivariateNormal, MultitaskMultivariateNormal\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Math, avoiding memory leak, and timing\n",
    "import math\n",
    "import gc\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb55500",
   "metadata": {},
   "source": [
    "## Full Input, Batched Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68991c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BatchedGP(ExactGP):\n",
    "    \"\"\"Class for creating batched Gaussian Process Regression models.  Ideal candidate if\n",
    "    using GPU-based acceleration such as CUDA for training.\n",
    "    Parameters:\n",
    "        train_x (torch.tensor): The training features used for Gaussian Process\n",
    "            Regression.  These features will take shape (B * YD, N, XD), where:\n",
    "                (i) B is the batch dimension - minibatch size\n",
    "                (ii) N is the number of data points per GPR - the neighbors considered\n",
    "                (iii) XD is the dimension of the features (d_state + d_action)\n",
    "                (iv) YD is the dimension of the labels (d_reward + d_state)\n",
    "            The features of train_x are tiled YD times along the first dimension.\n",
    "        train_y (torch.tensor): The training labels used for Gaussian Process\n",
    "            Regression.  These features will take shape (B * YD, N), where:\n",
    "                (i) B is the batch dimension - minibatch size\n",
    "                (ii) N is the number of data points per GPR - the neighbors considered\n",
    "                (iii) YD is the dimension of the labels (d_reward + d_state)\n",
    "            The features of train_y are stacked.\n",
    "        likelihood (gpytorch.likelihoods.GaussianLikelihood): A likelihood object\n",
    "            used for training and predicting samples with the BatchedGP model.\n",
    "        shape (int):  The batch shape used for creating this BatchedGP model.\n",
    "            This corresponds to the number of samples we wish to interpolate.\n",
    "        output_device (str):  The device on which the GPR will be trained on.\n",
    "        use_ard (bool):  Whether to use Automatic Relevance Determination (ARD)\n",
    "            for the lengthscale parameter, i.e. a weighting for each input dimension.\n",
    "            Defaults to False.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_x, train_y, likelihood, shape, output_device, use_ard=False):\n",
    "\n",
    "        # Run constructor of superclass\n",
    "        super(BatchedGP, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        # Determine if using ARD\n",
    "        ard_num_dims = None\n",
    "        if use_ard:\n",
    "            ard_num_dims = train_x.shape[-1]\n",
    "\n",
    "        # Create the mean and covariance modules\n",
    "        self.shape = torch.Size([shape])\n",
    "        self.mean_module = ConstantMean(batch_shape=self.shape)\n",
    "        self.base_kernel = RBFKernel(batch_shape=self.shape,\n",
    "                                        ard_num_dims=ard_num_dims)\n",
    "        self.covar_module = ScaleKernel(self.base_kernel,\n",
    "                                        batch_shape=self.shape,\n",
    "                                        output_device=output_device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass method for making predictions through the model.  The\n",
    "        mean and covariance are each computed to produce a MV distribution.\n",
    "        Parameters:\n",
    "            x (torch.tensor): The tensor for which we predict a mean and\n",
    "                covariance used the BatchedGP model.\n",
    "        Returns:\n",
    "            mv_normal (gpytorch.distributions.MultivariateNormal): A Multivariate\n",
    "                Normal distribution with parameters for mean and covariance computed\n",
    "                at x.\n",
    "        \"\"\"\n",
    "        mean_x = self.mean_module(x)  # Compute the mean at x\n",
    "        covar_x = self.covar_module(x)  # Compute the covariance at x\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0a6d87",
   "metadata": {},
   "source": [
    "## Factored Kernel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c8a6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeBatchedGP(ExactGP):\n",
    "    \"\"\"Class for creating batched Gaussian Process Regression models.  Ideal candidate if\n",
    "    using GPU-based acceleration such as CUDA for training.\n",
    "    This kernel produces a composite kernel that multiplies actions times states,\n",
    "    i.e. we have a different kernel for both the actions and states.  In turn,\n",
    "    the composite kernel is then multiplied by a Scale kernel.\n",
    "    Parameters:\n",
    "        train_x (torch.tensor): The training features used for Gaussian Process\n",
    "            Regression.  These features will take shape (B * YD, N, XD), where:\n",
    "                (i) B is the batch dimension - minibatch size\n",
    "                (ii) N is the number of data points per GPR - the neighbors considered\n",
    "                (iii) XD is the dimension of the features (d_state + d_action)\n",
    "                (iv) YD is the dimension of the labels (d_reward + d_state)\n",
    "            The features of train_x are tiled YD times along the first dimension.\n",
    "        train_y (torch.tensor): The training labels used for Gaussian Process\n",
    "            Regression.  These features will take shape (B * YD, N), where:\n",
    "                (i) B is the batch dimension - minibatch size\n",
    "                (ii) N is the number of data points per GPR - the neighbors considered\n",
    "                (iii) YD is the dimension of the labels (d_reward + d_state)\n",
    "            The features of train_y are stacked.\n",
    "        likelihood (gpytorch.likelihoods.GaussianLikelihood): A likelihood object\n",
    "            used for training and predicting samples with the BatchedGP model.\n",
    "        shape (int):  The batch shape used for creating this BatchedGP model.\n",
    "            This corresponds to the number of samples we wish to interpolate.\n",
    "        output_device (str):  The device on which the GPR will be trained on.\n",
    "        use_ard (bool):  Whether to use Automatic Relevance Determination (ARD)\n",
    "            for the lengthscale parameter, i.e. a weighting for each input dimension.\n",
    "            Defaults to False.\n",
    "        ds (int): If using a composite kernel, ds specifies the dimensionality of\n",
    "            the state.  Only applicable if composite_kernel is True.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_x, train_y, likelihood, shape, output_device,\n",
    "                 use_ard=False, ds=None):\n",
    "\n",
    "        # Run constructor of superclass\n",
    "        super(CompositeBatchedGP, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        # Check if ds is None, and if not, set\n",
    "        if ds is None:\n",
    "            raise Exception(\"No dimension for state specified.  Please specify ds.\")\n",
    "        self.ds = ds\n",
    "\n",
    "        # Set active dimensions\n",
    "        state_dims = torch.tensor([i for i in range(0, ds)])\n",
    "        action_dims = torch.tensor([i for i in range(ds, train_x.shape[-1])])\n",
    "\n",
    "        # Determine if using ARD\n",
    "        state_ard_num_dims = None\n",
    "        action_ard_num_dims = None\n",
    "        if use_ard:\n",
    "            state_ard_num_dims = ds\n",
    "            action_ard_num_dims = train_x.shape[-1] - ds\n",
    "\n",
    "        # Create the mean and covariance modules\n",
    "        self.shape = torch.Size([shape])\n",
    "\n",
    "        # Construct mean module\n",
    "        self.mean_module = ConstantMean(batch_shape=self.shape)\n",
    "\n",
    "        # Construct state kernel\n",
    "        self.state_base_kernel = RBFKernel(batch_shape=self.shape,\n",
    "                                              active_dims=state_dims,\n",
    "                                              ard_num_dims=state_ard_num_dims)\n",
    "\n",
    "        # Construct action kernel\n",
    "        self.action_base_kernel = RBFKernel(batch_shape=self.shape,\n",
    "                                               active_dims=action_dims,\n",
    "                                               ard_num_dims=action_ard_num_dims)\n",
    "\n",
    "        # Construct composite kernel\n",
    "        self.composite_kernel = self.state_base_kernel * self.action_base_kernel\n",
    "        self.covar_module = ScaleKernel(self.composite_kernel,\n",
    "                                        batch_shape=self.shape,\n",
    "                                        output_device=output_device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass method for making predictions through the model.  The\n",
    "        mean and covariance are each computed to produce a MV distribution.\n",
    "        Parameters:\n",
    "            x (torch.tensor): The tensor for which we predict a mean and\n",
    "                covariance used the BatchedGP model.\n",
    "        Returns:\n",
    "            mv_normal (gpytorch.distributions.MultivariateNormal): A Multivariate\n",
    "                Normal distribution with parameters for mean and covariance computed\n",
    "                at x.\n",
    "        \"\"\"\n",
    "        # Compute mean and covariance in batched form\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6bc29d",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a048b208",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Zs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess batch data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m B, N, XD \u001b[38;5;241m=\u001b[39m \u001b[43mZs\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      3\u001b[0m YD \u001b[38;5;241m=\u001b[39m Ys\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      4\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m B \u001b[38;5;241m*\u001b[39m YD\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Zs' is not defined"
     ]
    }
   ],
   "source": [
    "# Preprocess batch data\n",
    "B, N, XD = Zs.shape\n",
    "YD = Ys.shape[-1]\n",
    "batch_shape = B * YD\n",
    "\n",
    "if use_cuda:  # If GPU available\n",
    "    output_device = torch.device('cuda:0')  # GPU\n",
    "\n",
    "# Format the training features - tile and reshape\n",
    "train_x = torch.tensor(Zs, device=output_device)\n",
    "train_x = train_x.repeat((YD, 1, 1))\n",
    "\n",
    "# Format the training labels - reshape\n",
    "train_y = torch.vstack(\n",
    "    [torch.tensor(Ys, device=output_device)[..., i] for i in range(YD)])\n",
    "# train_x.shape\n",
    "# --> (B*D, N, C)\n",
    "# train_y.shape\n",
    "# --> (B*D, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a78c7c",
   "metadata": {},
   "source": [
    "## Training Batched, Multidimensional GPR Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22bf3417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gp_batched_scalar(Zs, Ys, use_cuda=False, epochs=10,\n",
    "                            lr=0.1, thr=0, use_ard=False, composite_kernel=False,\n",
    "                            ds=None, global_hyperparams=False,\n",
    "                            model_hyperparams=None):\n",
    "    \"\"\"Computes a Gaussian Process object using GPyTorch. Each outcome is\n",
    "    modeled as a single scalar outcome.\n",
    "    Parameters:\n",
    "        Zs (np.array): Array of inputs of expanded shape (B, N, XD), where B is\n",
    "            the size of the minibatch, N is the number of data points in each\n",
    "            GP (the number of neighbors we consider in IER), and XD is the\n",
    "            dimensionality of the state-action space of the environment.\n",
    "        Ys (np.array): Array of predicted values of shape (B, N, YD), where B is the\n",
    "            size of the minibatch and N is the number of data points in each\n",
    "            GP (the number of neighbors we consider in IER), and YD is the\n",
    "            dimensionality of the state-reward space of the environment.\n",
    "        use_cuda (bool): Whether to use CUDA for GPU acceleration with PyTorch\n",
    "            during the optimization step.  Defaults to False.\n",
    "        epochs (int):  The number of epochs to train the batched GPs over.\n",
    "            Defaults to 10.\n",
    "        lr (float):  The learning rate to use for the Adam optimizer to train\n",
    "            the batched GPs.\n",
    "        thr (float):  The mll threshold at which to stop training.  Defaults to 0.\n",
    "        use_ard (bool):  Whether to use Automatic Relevance Determination (ARD)\n",
    "            for the lengthscale parameter, i.e. a weighting for each input dimension.\n",
    "            Defaults to False.\n",
    "        composite_kernel (bool):  Whether to use a composite kernel that computes\n",
    "            the product between states and actions to compute the variance of y.\n",
    "        ds (int): If using a composite kernel, ds specifies the dimensionality of\n",
    "            the state.  Only applicable if composite_kernel is True.\n",
    "        global_hyperparams (bool):  Whether to use a single set of hyperparameters\n",
    "            over an entire model.  Defaults to False.\n",
    "        model_hyperparams (dict):  A dictionary of hyperparameters to use for\n",
    "            initializing a model.  Defaults to None.\n",
    "    Returns:\n",
    "        model (BatchedGP): A GPR model of BatchedGP type with which to generate\n",
    "            synthetic predictions of rewards and next states.\n",
    "        likelihood (GaussianLikelihood): A likelihood object used for training\n",
    "            and predicting samples with the BatchedGP model.\n",
    "    \"\"\"\n",
    "    # Preprocess batch data\n",
    "    B, N, XD = Zs.shape\n",
    "    YD = Ys.shape[-1]\n",
    "    batch_shape = B * YD\n",
    "\n",
    "    if use_cuda:  # If GPU available\n",
    "        output_device = torch.device('cuda:0')  # GPU\n",
    "\n",
    "    # Format the training features - tile and reshape\n",
    "    train_x = torch.tensor(Zs, device=output_device)\n",
    "    train_x = train_x.repeat((YD, 1, 1))\n",
    "\n",
    "    # Format the training labels - reshape\n",
    "    train_y = torch.vstack(\n",
    "        [torch.tensor(Ys, device=output_device)[..., i] for i in range(YD)])\n",
    "\n",
    "    # initialize likelihood and model\n",
    "    likelihood = GaussianLikelihood(batch_shape=torch.Size([batch_shape]))\n",
    "\n",
    "    # Determine which type of kernel to use\n",
    "    if composite_kernel:\n",
    "        model = CompositeBatchedGP(train_x, train_y, likelihood, batch_shape,\n",
    "                          output_device, use_ard=use_ard, ds=ds)\n",
    "    else:\n",
    "        model = BatchedGP(train_x, train_y, likelihood, batch_shape,\n",
    "                          output_device, use_ard=use_ard)\n",
    "\n",
    "    # Initialize the model with hyperparameters\n",
    "    if model_hyperparams is not None:\n",
    "        model.initialize(**model_hyperparams)\n",
    "\n",
    "    # Determine if we need to optimize hyperparameters\n",
    "    if global_hyperparams:\n",
    "        if use_cuda:  # Send everything to GPU for training\n",
    "            model = model.cuda().eval()\n",
    "\n",
    "            # Empty the cache from GPU\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()  # NOTE: Critical to avoid GPU leak\n",
    "            del train_x, train_y, Zs, Ys, likelihood\n",
    "\n",
    "        return model, model.likelihood\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    if use_cuda:  # Send everything to GPU for training\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "        train_x = train_x.cuda()\n",
    "        train_y = train_y.cuda()\n",
    "        mll = mll.cuda()\n",
    "\n",
    "    def epoch_train(j):\n",
    "        \"\"\"Helper function for running training in the optimization loop.  Note\n",
    "        that the model and likelihood are updated outside of this function as well.\n",
    "        Parameters:\n",
    "            j (int):  The epoch number.\n",
    "        Returns:\n",
    "            item_loss (float):  The numeric representation (detached from the\n",
    "                computation graph) of the loss from the jth epoch.\n",
    "        \"\"\"\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        output = model(train_x)  # Forwardpass\n",
    "        loss = -mll(output, train_y).sum()  # Compute ind. losses + aggregate\n",
    "        loss.backward()  # Backpropagate gradients\n",
    "        item_loss = loss.item()  # Extract loss (detached from comp. graph)\n",
    "        optimizer.step()  # Update weights\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        gc.collect()  # NOTE: Critical to avoid GPU leak\n",
    "        return item_loss\n",
    "\n",
    "    # Run the optimization loop\n",
    "    for i in range(epochs):\n",
    "        loss_i = epoch_train(i)\n",
    "        if i % 10 == 0:\n",
    "            print(\"LOSS EPOCH {}: {}\".format(i, loss_i))\n",
    "        if loss_i < thr:  # If we reach a certain loss threshold, stop training\n",
    "            break\n",
    "\n",
    "    # Empty the cache from GPU\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e4501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
